线性回归
=================

有监督学习
-----------
输入样本有标签

无监督学习
-----------
输入样本没有标签

泛化能力
---------
是指模型对于新样本的识别能力

欠拟合
-------
模型在训练数据集表现良好，测试数据集表现差

过拟合
-------
模型过于简单，没法很好的学习到数据背后的规律

偏差
---
是指预测值与真实值的差异，即模型精度

方差
---
是指模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性

交叉验证
-------
第一种是简单交叉验证，所谓的简单，是和其他交叉验证方法相对而言的。首先，我们随机的将样本数据分为两部分（比如： 70%的训练集，30%的测试集），然后用训练集来训练模型，在测试集上验证模型及参数。接着，我们再把样本打乱，重新选择训练集和测试集，继续训练数据和检验模型。最后我们选择损失函数评估最优的模型和参数。　

第二种是S折交叉验证（S-Folder Cross Validation）。和第一种方法不同，S折交叉验证会把样本数据随机的分成S份，每次随机的选择S-1份作为训练集，剩下的1份做测试集。当这一轮完成后，重新随机选择S-1份来训练数据。若干轮（小于S）之后，选择损失函数评估最优的模型和参数。

第三种是留一交叉验证（Leave-one-out Cross Validation），它是第二种情况的特例，此时S等于样本数N，这样对于N个样本，每次选择N-1个样本来训练数据，留一个样本来验证模型预测的好坏。此方法主要用于样本量非常少的情况，比如对于普通适中问题，N小于50时，我一般采用留一交叉验证。

sklearn.model_selection  train_test_spilt

代价函数
-------
![image](https://github.com/zzcNEU/ml/blob/master/%E5%88%9D%E7%BA%A7%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86/%E5%9B%BE%E7%89%87/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0.png)

优化方法
-------
梯度下降
-------
梯度下降法是最早最简单，也是最为常用的最优化方法。梯度下降法实现简单，当目标函数是凸函数时，梯度下降法的解是全局解。一般情况下，其解不保证是全局最优解，梯度下降法的速度也未必是最快的。梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是”最速下降法“。最速下降法越接近目标值，步长越小，前进越慢。

牛顿法
------
牛顿法的基本思想是利用迭代点处的一阶导数（梯度）和二阶导数（Hessian）对目标函数进行二次函数近似，然后把二次模型的极小点作为新的迭代点，并不断重复这一过程，直至求得满足精度的近似极小值

评估指标
-------
均方误差（MSE），用 真实值-预测值 然后平方之后求和平均。其实就是线性回归的损失函数。
均方根误差（RMSE），MSE开根号
平均绝对误差（MAE），将上面的平方和根号换成绝对值。

sklearn参数解释
--------------
from sklearn.linear_model import LinearRegression

[sklearn官网函数解释]https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn-linear-model-linearregression



